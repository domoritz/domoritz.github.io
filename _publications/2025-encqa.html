---
layout: publication
year: 2025
title: "EncQA: Benchmarking Vision-Language Models on Visual Encodings for Charts"
authors:
  - Kushin Mukherjee
  - Donghao Ren
  - Dominik Moritz
  - Yannick Assogba
venue: VIS
venue_location: Vienna, Austria
venue_url: https://ieeevis.org/year/2025/welcome
venue_tags:
  - VIS
type:
  - Conference
tags:
  - Vision-Language Models
  - Chart Understanding
  - Visualization
  - Machine Learning
link: https://github.com/apple/ml-encqa
arxiv: "2508.04650"
---

Multimodal vision-language models (VLMs) continue to achieve ever-improving scores on chart understanding benchmarks. Yet, we find that this progress does not fully capture the breadth of visual reasoning capabilities essential for interpreting charts. We introduce EncQA, a novel benchmark informed by the visualization literature, designed to provide systematic coverage of visual encodings and analytic tasks that are crucial for chart understanding. EncQA provides 2,076 synthetic question-answer pairs, enabling balanced coverage of six visual encoding channels (position, length, area, color quantitative, color nominal, and shape) and eight tasks (find extrema, retrieve value, find anomaly, filter values, compute derived value exact, compute derived value relative, correlate values, and correlate values relative). Our evaluation of 9 state-of-the-art VLMs reveals that performance varies significantly across encodings within the same task, as well as across tasks. Contrary to expectations, we observe that performance does not improve with model size for many task-encoding pairs. Our results suggest that advancing chart understanding requires targeted strategies addressing specific visual reasoning gaps, rather than solely scaling up model or dataset size.
